{"cells":[{"cell_type":"markdown","source":["-sandbox\n# Automation with runnable notebooks\n\nNotebooks are one--and not the only--way of interacting with the Spark and Databricks environment.  Notebooks can be executed independently and as recurring jobs.  They can also be exported and versioned using git.  Python files and Scala/Java jars can be executed against a Databricks cluster as well, allowing full integration with a developer's normal workflow.  Since notebooks can be executed like code files and compiled binaries, they offer a way of building production pipelines.\n\nFunctional programming design principles aid in thinking about pipelines.  In functional programming, your code always has known inputs and outputs without any side effects.  In the case of automating notebooks, coding notebooks in this way helps reduce any unintended side effects where each stage in a pipeline can operate independently from the rest.\n\nMore complex workflows using notebooks require managing dependencies between tasks and passing parameters into notebooks.  Dependency management can done by chaining notebooks together, for instance to run reporting logic after having completed a database write. Sometimes, when these pipelines become especially complex, chaining notebooks together isn't sufficient. In those cases, scheduling with Apache Airflow has become the preferred solution. Notebook widgets can be used to pass parameters to a notebook when the parameters are determined at runtime.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-3/notebook-workflows.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Widgets\n\nWidgets allow for the customization of notebooks without editing the code itself.  They also allow for passing parameters into notebooks.  There are 4 types of widgets:\n\n| Type          | Description                                                                                        |\n|:--------------|:---------------------------------------------------------------------------------------------------|\n| `text`        | Input a value in a text box.                                                                       |\n| `dropdown`    | Select a value from a list of provided values.                                                     |\n| `combobox`    | Combination of text and dropdown. Select a value from a provided list or input one in the text box.|\n| `multiselect` | Select one or more values from a list of provided values.                                          |\n\nWidgets are Databricks utility functions that can be accessed using the `dbutils.widgets` package and take a name, default value, and values (if not a `text` widget).\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Check out <a href=\"https://docs.azuredatabricks.net/user-guide/notebooks/widgets.html#id1\" target=\"_blank\">the Databricks documentation on widgets for additional information </a>"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"MyWidget\", \"1\", [str(x) for x in range(1, 5)])"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Notice the widget created at the top of the screen.  Choose a number from the dropdown menu.  Now, bring that value into your code using the `get` method."],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.get(\"MyWidget\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Clear the widgets using either `remove()` or `removeAll()`"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.removeAll()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["While great for adding parameters to notebooks and dashboards, widgets also allow us to pass parameters into notebooks when we run them like a Python or JAR file."],"metadata":{}},{"cell_type":"markdown","source":["### Running Notebooks\n\nThere are two options for running notebooks.  The first is \n- Using `dbutils.notebook.run(\"<path>\", \"<timeout>\")`.  \n- Using `%run` magic.\n\nThis variable is not passed into our current environment.  The difference between `dbutils.notebook.run()` and `%run` is that the parent notebook will inherit variables from the ran notebook with `%run`.\n\nNotebook widgets allow to pass parameters into notebooks.  This can be done in the form of a dictionary that maps the widget name to a value as a `string`.\n\nThe execution record of the ran notebook can be reviewed in the *Jobs* section of the workspace.\n\nRunning notebooks can allow for more advanced workflows in the following ways:<br><br>\n\n* Managing **dependencies** can be ensured by running a notebook that triggers other notebooks in the desired order\n* Setting **timeouts** ensures that jobs have a set limit on when they must either complete or fail\n* **Retry logic** ensures that fleeting failures do not prevent the proper execution of a notebook\n* **Data can passed** between notebooks by saving the data to a blob store or table and passing the path as an exit parameter"],"metadata":{}},{"cell_type":"markdown","source":["## An example of a simple ETL job\n\n1. Takes three parameters: \n  - Azure Storage account\n  - Azure Blob Storage container with CSV files\n  - Output pathname\n  - Path to Spark ML classification model\n1. Reads the CSV files to DataFrame \n1. Applies the Spark ML classification model adding a prediction column to the input DataFrame\n1. Writes the result to DBFS as a parquet file \n1. Exits with the input path and output path as a result"],"metadata":{}},{"cell_type":"code","source":["%fs ls \"wasbs://models@azureailabs.blob.core.windows.net/churn_classifier\""],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import json\n\nSTORAGE_ACCOUNT = \"azureailabs\"\nCONTAINER = \"churn\"\nOUTPUT_PATH = \"/datasets/churn.parquet\"\nML_PATH = \"wasbs://models@azureailabs.blob.core.windows.net/churn_classifier\"\n\n\nresult = dbutils.notebook.run(\"./Runnable/score\", 120, \n                     {\"STORAGE_ACCOUNT\": STORAGE_ACCOUNT,\n                      \"CONTAINER\": CONTAINER,\n                      \"ML_PATH\": ML_PATH,\n                      \"OUTPUT_PATH\": OUTPUT_PATH })\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["print(json.loads(result))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["scoredDF = spark.read.parquet(OUTPUT_PATH)\ndisplay(scoredDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"02 - Runnable-Notebooks","notebookId":126212991376739},"nbformat":4,"nbformat_minor":0}
