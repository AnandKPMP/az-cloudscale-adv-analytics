{"cells":[{"cell_type":"markdown","source":["## Get parameters for this Notebook\n\nUpdate parameters before you run codes\n\nYou need blob Account Name and Key"],"metadata":{}},{"cell_type":"code","source":["# Creating widgets for leveraging parameters, and printing the parameters\n\ndbutils.widgets.text(\"dirpath\", \"workshop\")\ndbutils.widgets.text(\"blobAccountName\", \"\")\ndbutils.widgets.text(\"blobAccountKey\", \"\")\ndbutils.widgets.text(\"blobContainer\", \"ingest\")\n\ndbutils.widgets.get(\"dirpath\")\ndirpath = getArgument(\"dirpath\")\n\ndbutils.widgets.get(\"blobAccountName\")\nblob = getArgument(\"blobAccountName\")\n\ndbutils.widgets.get(\"blobAccountKey\")\nblobkey = getArgument(\"blobAccountKey\")\n\ndbutils.widgets.get(\"blobContainer\")\ncontainer = getArgument(\"blobContainer\")\n\nprint(dirpath)\nprint(blob)\nprint(blobkey)\nprint(container)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create folder and mount blob to the folder\nfullpath=\"/mnt/\"+dirpath\ndbutils.fs.mkdirs(fullpath)\nprint(fullpath)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Mount blob to the folder\ndbutils.fs.mount(source = \"wasbs://\"+container+\"@\"+blob+\".blob.core.windows.net\",mount_point = fullpath,extra_configs = {\"fs.azure.account.key.\"+blob+\".blob.core.windows.net\":blobkey})"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%python\n\ndf = sqlContext.read.format('csv').options(header='true', inferSchema='true').load(fullpath+\"/customerchurnsource.csv\")\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%r\nlibrary(SparkR)\n\ndfr <- read.df(\"/mnt/workshop/customerchurnsource.csv\", source = \"csv\", header=\"true\", inferSchema = \"true\")\n\ndisplay(dfr)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala \n\nval fullpath = \"/mnt/workshop\"\nval dfs = sqlContext.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(fullpath+\"/customerchurnsource.csv\")\n\ndisplay(dfs)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\n-- mode \"FAILFAST\" will abort file parsing with a RuntimeException if any malformed lines are encountered\nDROP TABLE IF EXISTS tblchurn;\n\nCREATE TABLE IF NOT EXISTS tblchurn\n  USING csv\n  OPTIONS (path \"mnt/workshop/customerchurnsource.csv\", header \"true\", mode \"FAILFAST\");\n  \nSELECT * FROM tblchurn"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Save data as CSV"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport csv"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["df=df.toPandas()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df = df.fillna(0)\ndf = df.drop_duplicates()\ndf = df.drop(['year','month'],1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(spark.createDataFrame(df))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/mnt/workshop/azmlstudio.csv\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Run Machine Learning with Sci-kit Learn"],"metadata":{}},{"cell_type":"code","source":["# Customer Churn Prediction\nimport pickle\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# One-Hot Encoding\ncolumns_to_encode = list(df.select_dtypes(include=['category','object']))\nfor column_to_encode in columns_to_encode:\n    dummies = pd.get_dummies(df[column_to_encode])\n    one_hot_col_names = []\n    for col_name in list(dummies.columns):\n        one_hot_col_names.append(column_to_encode + '_' + col_name)\n    dummies.columns = one_hot_col_names\n    df = df.drop(column_to_encode, axis=1)\n    df = df.join(dummies)    \n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["model = GaussianNB()\n\nrandom_seed = 42\ntrain, test = train_test_split(df, random_state = random_seed, test_size = 0.3)\n\ntarget = train['churn'].values\ntrain = train.drop('churn', 1)\ntrain = train.values\nmodel.fit(train, target)\n\nexpected = test['churn'].values\ntest = test.drop('churn', 1)\npredicted = model.predict(test)\nprint(\"Naive Bayes Classification Accuracy\", accuracy_score(expected, predicted))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\ndt.fit(train, target)\npredicted = dt.predict(test)\nprint(\"Decision Tree Classification Accuracy\", accuracy_score(expected, predicted))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["pickle.dump(model, open(\"model.pkl\", \"wb\"))"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"02.datawrangling","notebookId":516688246035869},"nbformat":4,"nbformat_minor":0}
