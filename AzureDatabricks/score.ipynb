{"cells":[{"cell_type":"code","source":["# Configure input parameters\n\ndbutils.widgets.text(\"STORAGE_ACCOUNT\", \"\")\ndbutils.widgets.text(\"CONTAINER\", \"\")\ndbutils.widgets.text(\"ML_PATH\", \"\")\ndbutils.widgets.text(\"ACCOUNT_KEY\", \"\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Set up connection to Azure Blob Storage\nSTORAGE_ACCOUNT = dbutils.widgets.get(\"STORAGE_ACCOUNT\").strip()\nCONTAINER = dbutils.widgets.get(\"CONTAINER\").strip()\n#wasbs://models@azureailabs.blob.core.windows.net/churn_classifier\nML_PATH = dbutils.widgets.get(\"ML_PATH\").strip() \nACCOUNT_KEY = dbutils.widgets.get(\"ACCOUNT_KEY\").strip()\n\nif ACCOUNT_KEY != \"\":\n  # Set up account access key\n  conf_key = \"fs.azure.account.key.{storage_acct}.blob.core.windows.net\".format(storage_acct=STORAGE_ACCOUNT)\n  spark.conf.set(conf_key, ACCOUNT_KEY)\n\nsource_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\nresult_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/{dirname}\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT,dirname=\"results\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Load data to score\nschema = StructType([\n  StructField(\"age\", DoubleType()),\n  StructField(\"annualincome\", DoubleType()),\n  StructField(\"calldroprate\", DoubleType()),\n  StructField(\"callfailurerate\", DoubleType()),\n  StructField(\"callingnum\", StringType()),\n  StructField(\"customerid\", StringType()),\n  StructField(\"customersuspended\",  StringType()),\n  StructField(\"education\",  StringType()),\n  StructField(\"gender\", StringType()),\n  StructField(\"homeowner\", StringType()),\n  StructField(\"maritalstatus\", StringType()),\n  StructField(\"monthlybilledamount\", DoubleType()),\n  StructField(\"noadditionallines\", StringType()),\n  StructField(\"numberofcomplaints\", DoubleType()),\n  StructField(\"numberofmonthunpaid\", DoubleType()),\n  StructField(\"numdayscontractequipmentplanexpiring\", DoubleType()),\n  StructField(\"occupation\", StringType()),\n  StructField(\"penaltytoswitch\", DoubleType()),\n  StructField(\"state\", StringType()),\n  StructField(\"totalminsusedinlastmonth\", DoubleType()),\n  StructField(\"unpaidbalance\", DoubleType()),\n  StructField(\"usesinternetservice\", StringType()),\n  StructField(\"usesvoiceservice\", StringType()),\n  StructField(\"percentagecalloutsidenetwork\", DoubleType()),\n  StructField(\"totalcallduration\", DoubleType()),\n  StructField(\"avgcallduration\", DoubleType()),\n  StructField(\"churn\", DoubleType()),\n  StructField(\"year\", DoubleType()),\n  StructField(\"month\", DoubleType())\n])\n\ndf = (spark.read\n     .option(\"header\", True)\n     .schema(schema)\n     .csv(source_str))\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.ml import PipelineModel\nfrom pyspark.sql.functions import col\n\n# Load churn classification model\npipelineModel = PipelineModel.load(ML_PATH)\n\n# Score input data\npredictions = pipelineModel.transform(df)\n\nscoredDF = predictions.select(col(\"customerid\").alias(\"customerid\"),col(\"callingnum\").alias(\"callingnum\"), col(\"prediction\").alias(\"churn_prediction\"))\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Save results to Parquet\nprint(\"Saving results to: \", result_str)\nscoredDF.write.mode(\"overwrite\").csv(result_str)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["import json\n\n# Return status\ndbutils.notebook.exit(json.dumps({\n    \"status\": \"OK\",\n    \"output_path\": result_str}))"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"score","notebookId":1741362454043471},"nbformat":4,"nbformat_minor":0}
